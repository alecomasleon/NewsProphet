{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-18T07:05:34.956765700Z",
     "start_time": "2024-02-18T07:05:34.948780100Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (12600, 30) y shape: (12600,)\n",
      "   n_tokens_title  n_tokens_content  n_unique_tokens  n_non_stop_words  \\\n",
      "0        0.947707         -0.439662         0.428161          0.002998   \n",
      "1       -0.684112         -0.329189        -0.117285         -0.230132   \n",
      "2       -0.684112         -0.488761        -0.049791         -0.040442   \n",
      "3       -0.140172          0.315234        -1.027334         -0.225009   \n",
      "4        1.491646         -0.200305        -0.756442          0.313212   \n",
      "\n",
      "   n_non_stop_unique_tokens  num_hrefs  num_imgs  num_videos  \\\n",
      "0                 -0.418972  -0.324104 -0.313340         0.0   \n",
      "1                 -0.319133  -0.584280 -0.313340         0.0   \n",
      "2                 -0.568731  -0.584280 -0.313340         0.0   \n",
      "3                  0.296542  -0.454192  0.058349         0.0   \n",
      "4                 -0.244254  -0.844455 -0.313340         0.0   \n",
      "\n",
      "   average_token_length  weekday_is_monday  ...  global_rate_negative_words  \\\n",
      "0             -0.186062                  1  ...                   -1.674196   \n",
      "1              0.390074                  1  ...                   -1.674196   \n",
      "2             -0.329677                  1  ...                   -1.674196   \n",
      "3             -1.096564                  1  ...                   -1.674196   \n",
      "4             -0.384493                  1  ...                   -1.674196   \n",
      "\n",
      "   rate_positive_words  rate_negative_words  avg_positive_polarity  \\\n",
      "0               1.6741              -1.6741              -3.531853   \n",
      "1               1.6741              -1.6741              -0.424823   \n",
      "2               1.6741              -1.6741               8.867751   \n",
      "3               1.6741              -1.6741              -2.101861   \n",
      "4               1.6741              -1.6741               1.562192   \n",
      "\n",
      "   max_positive_polarity  avg_negative_polarity  min_negative_polarity  \\\n",
      "0              -3.280911               0.121589               9.744565   \n",
      "1              -2.643867               0.121589               9.744565   \n",
      "2              -0.673668               0.121589               9.744565   \n",
      "3              -0.673668               0.121589               9.744565   \n",
      "4              -0.673668               0.121589               9.744565   \n",
      "\n",
      "   title_subjectivity  title_sentiment_polarity  abs_title_sentiment_polarity  \n",
      "0            0.730842                 -1.002120                      0.163599  \n",
      "1           -0.806824                 -0.291287                     -0.650595  \n",
      "2           -0.806824                 -0.291287                     -0.650595  \n",
      "3           -0.806824                 -0.291287                     -0.650595  \n",
      "4            0.591055                  0.225684                     -0.058454  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "%run new_preprocess.ipynb\n",
    "def split(X, y, test_size, val_size):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=1)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_size/(1-test_size), random_state=1)\n",
    "\n",
    "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
    "\n",
    "df = pd.read_csv('cleaned_extracted_data.csv')\n",
    "df = df.drop('Unnamed: 0', axis=1)\n",
    "X = df.drop('shares', axis=1)\n",
    "y = df['shares']\n",
    "print(f\"X shape: {X.shape}\", f\"y shape: {y.shape}\")\n",
    "print(X.head())\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = split(X, y, 0.1, 0.3)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:53:34.893451800Z",
     "start_time": "2024-02-18T06:53:34.283942Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'uci_id': 332, 'name': 'Online News Popularity', 'repository_url': 'https://archive.ics.uci.edu/dataset/332/online+news+popularity', 'data_url': 'https://archive.ics.uci.edu/static/public/332/data.csv', 'abstract': 'This dataset summarizes a heterogeneous set of features about articles published by Mashable in a period of two years. The goal is to predict the number of shares in social networks (popularity).', 'area': 'Business', 'tasks': ['Classification', 'Regression'], 'characteristics': ['Multivariate'], 'num_instances': 39797, 'num_features': 58, 'feature_types': ['Integer', 'Real'], 'demographics': [], 'target_col': [' shares'], 'index_col': ['url'], 'has_missing_values': 'no', 'missing_values_symbol': None, 'year_of_dataset_creation': 2015, 'last_updated': 'Thu Feb 15 2024', 'dataset_doi': '10.24432/C5NS3V', 'creators': ['Kelwin Fernandes', 'Pedro Vinagre', 'Paulo Cortez', 'Pedro Sernadela'], 'intro_paper': {'title': 'A Proactive Intelligent Decision Support System for Predicting the Popularity of Online News', 'authors': 'Kelwin Fernandes, Pedro Vinagre, P. Cortez', 'published_in': 'Portuguese Conference on Artificial Intelligence', 'year': 2015, 'url': 'https://www.semanticscholar.org/paper/A-Proactive-Intelligent-Decision-Support-System-for-Fernandes-Vinagre/ad7f3da7a5d6a1e18cc5a176f18f52687b912fea', 'doi': None}, 'additional_info': {'summary': '* The articles were published by Mashable (www.mashable.com) and their content as the rights to reproduce it belongs to them. Hence, this dataset does not share the original content but some statistics associated with it. The original content be publicly accessed and retrieved using the provided urls.\\r\\n* Acquisition date: January 8, 2015\\r\\n* The estimated relative performance values were estimated by the authors using a Random Forest classifier and a rolling windows as assessment method.  See their article for more details on how the relative performance values were set.', 'purpose': None, 'funded_by': None, 'instances_represent': None, 'recommended_data_splits': None, 'sensitive_data': None, 'preprocessing_description': None, 'variable_info': \"Number of Attributes: 61 (58 predictive attributes, 2 non-predictive, 1 goal field)\\r\\n\\r\\nAttribute Information:\\r\\n     0. url:                           URL of the article (non-predictive)\\r\\n     1. timedelta:                     Days between the article publication and the dataset acquisition (non-predictive)\\r\\n     2. n_tokens_title:                Number of words in the title\\r\\n     3. n_tokens_content:              Number of words in the content\\r\\n     4. n_unique_tokens:               Rate of unique words in the content\\r\\n     5. n_non_stop_words:              Rate of non-stop words in the content\\r\\n     6. n_non_stop_unique_tokens:      Rate of unique non-stop words in the content\\r\\n     7. num_hrefs:                     Number of links\\r\\n     8. num_self_hrefs:                Number of links to other articles published by Mashable\\r\\n     9. num_imgs:                      Number of images\\r\\n    10. num_videos:                    Number of videos\\r\\n    11. average_token_length:          Average length of the words in the content\\r\\n    12. num_keywords:                  Number of keywords in the metadata\\r\\n    13. data_channel_is_lifestyle:     Is data channel 'Lifestyle'?\\r\\n    14. data_channel_is_entertainment: Is data channel 'Entertainment'?\\r\\n    15. data_channel_is_bus:           Is data channel 'Business'?\\r\\n    16. data_channel_is_socmed:        Is data channel 'Social Media'?\\r\\n    17. data_channel_is_tech:          Is data channel 'Tech'?\\r\\n    18. data_channel_is_world:         Is data channel 'World'?\\r\\n    19. kw_min_min:                    Worst keyword (min. shares)\\r\\n    20. kw_max_min:                    Worst keyword (max. shares)\\r\\n    21. kw_avg_min:                    Worst keyword (avg. shares)\\r\\n    22. kw_min_max:                    Best keyword (min. shares)\\r\\n    23. kw_max_max:                    Best keyword (max. shares)\\r\\n    24. kw_avg_max:                    Best keyword (avg. shares)\\r\\n    25. kw_min_avg:                    Avg. keyword (min. shares)\\r\\n    26. kw_max_avg:                    Avg. keyword (max. shares)\\r\\n    27. kw_avg_avg:                    Avg. keyword (avg. shares)\\r\\n    28. self_reference_min_shares:     Min. shares of referenced articles in Mashable\\r\\n    29. self_reference_max_shares:     Max. shares of referenced articles in Mashable\\r\\n    30. self_reference_avg_sharess:    Avg. shares of referenced articles in Mashable\\r\\n    31. weekday_is_monday:             Was the article published on a Monday?\\r\\n    32. weekday_is_tuesday:            Was the article published on a Tuesday?\\r\\n    33. weekday_is_wednesday:          Was the article published on a Wednesday?\\r\\n    34. weekday_is_thursday:           Was the article published on a Thursday?\\r\\n    35. weekday_is_friday:             Was the article published on a Friday?\\r\\n    36. weekday_is_saturday:           Was the article published on a Saturday?\\r\\n    37. weekday_is_sunday:             Was the article published on a Sunday?\\r\\n    38. is_weekend:                    Was the article published on the weekend?\\r\\n    39. LDA_00:                        Closeness to LDA topic 0\\r\\n    40. LDA_01:                        Closeness to LDA topic 1\\r\\n    41. LDA_02:                        Closeness to LDA topic 2\\r\\n    42. LDA_03:                        Closeness to LDA topic 3\\r\\n    43. LDA_04:                        Closeness to LDA topic 4\\r\\n    44. global_subjectivity:           Text subjectivity\\r\\n    45. global_sentiment_polarity:     Text sentiment polarity\\r\\n    46. global_rate_positive_words:    Rate of positive words in the content\\r\\n    47. global_rate_negative_words:    Rate of negative words in the content\\r\\n    48. rate_positive_words:           Rate of positive words among non-neutral tokens\\r\\n    49. rate_negative_words:           Rate of negative words among non-neutral tokens\\r\\n    50. avg_positive_polarity:         Avg. polarity of positive words\\r\\n    51. min_positive_polarity:         Min. polarity of positive words\\r\\n    52. max_positive_polarity:         Max. polarity of positive words\\r\\n    53. avg_negative_polarity:         Avg. polarity of negative  words\\r\\n    54. min_negative_polarity:         Min. polarity of negative  words\\r\\n    55. max_negative_polarity:         Max. polarity of negative  words\\r\\n    56. title_subjectivity:            Title subjectivity\\r\\n    57. title_sentiment_polarity:      Title polarity\\r\\n    58. abs_title_subjectivity:        Absolute subjectivity level\\r\\n    59. abs_title_sentiment_polarity:  Absolute polarity level\\r\\n    60. shares:                        Number of shares (target)\", 'citation': None}}\n",
      "                             name     role         type demographic  \\\n",
      "0                             url       ID  Categorical        None   \n",
      "1                       timedelta    Other   Continuous        None   \n",
      "2                  n_tokens_title  Feature   Continuous        None   \n",
      "3                n_tokens_content  Feature   Continuous        None   \n",
      "4                 n_unique_tokens  Feature   Continuous        None   \n",
      "..                            ...      ...          ...         ...   \n",
      "56             title_subjectivity  Feature   Continuous        None   \n",
      "57       title_sentiment_polarity  Feature   Continuous        None   \n",
      "58         abs_title_subjectivity  Feature   Continuous        None   \n",
      "59   abs_title_sentiment_polarity  Feature   Continuous        None   \n",
      "60                         shares   Target      Integer        None   \n",
      "\n",
      "   description units missing_values  \n",
      "0         None  None             no  \n",
      "1         None  None             no  \n",
      "2         None  None             no  \n",
      "3         None  None             no  \n",
      "4         None  None             no  \n",
      "..         ...   ...            ...  \n",
      "56        None  None             no  \n",
      "57        None  None             no  \n",
      "58        None  None             no  \n",
      "59        None  None             no  \n",
      "60        None  None             no  \n",
      "\n",
      "[61 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# load data without preprocessing\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "\n",
    "# fetch dataset\n",
    "online_news_popularity = fetch_ucirepo(id=332)\n",
    "\n",
    "# data (as pandas dataframes)\n",
    "X_raw = online_news_popularity.data.features\n",
    "y_raw = online_news_popularity.data.targets\n",
    "\n",
    "# metadata\n",
    "print(online_news_popularity.metadata)\n",
    "# variable information\n",
    "print(online_news_popularity.variables)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:53:50.759454Z",
     "start_time": "2024-02-18T06:53:45.219477100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# split unpreprocessed data into training, validation and test sets\n",
    "X_train_raw, y_train_raw, X_val_raw, y_val_raw, X_test_raw, y_test_raw = split(X_raw, y_raw,test_size=0.2, val_size=0.3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T03:53:31.468782900Z",
     "start_time": "2024-02-18T03:53:31.439723900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rodcs\\.conda\\envs\\data_science_tests\\Lib\\site-packages\\sklearn\\base.py:1152: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  return fit_method(estimator, *args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": "RandomForestRegressor(n_estimators=72, random_state=42)",
      "text/html": "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=72, random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=72, random_state=42)</pre></div></div></div></div></div>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a random forest regressor model\n",
    "rf = RandomForestRegressor(n_estimators=72, random_state=42) # n_estimators is the number of trees in the forest\n",
    "rf.fit(X_train, y_train)\n",
    "# create a random forest regressor model\n",
    "rf_raw = RandomForestRegressor(n_estimators=72, random_state=42) # n_estimators is the number of trees in the forest\n",
    "rf_raw.fit(X_train_raw, y_train_raw)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:58:03.977314700Z",
     "start_time": "2024-02-18T06:54:48.496768100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error for normalised data: 0.9328387286124296\n",
      "Mean Absolute Error for normalised data: 0.2483369377429987\n",
      "R^2 score for normalised data: -0.054696284553577446\n",
      "Mean Squared Error for raw data: 160002443.72788826\n",
      "Mean Absolute Error for raw data: 3513.2590416736734\n",
      "R^2 score for raw data: -0.03922504724847964\n"
     ]
    }
   ],
   "source": [
    "# Caclulate performance using the validation set\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "y_pred = rf.predict(X_val)\n",
    "mse = mean_squared_error(y_val, y_pred)\n",
    "mae = mean_absolute_error(y_val, y_pred)\n",
    "score = rf.score(X_val, y_val)\n",
    "print(f'Mean Squared Error for normalised data: {mse}')\n",
    "print(f'Mean Absolute Error for normalised data: {mae}')\n",
    "print(f'R^2 score for normalised data: {score}')\n",
    "\n",
    "y_pred_raw = rf_raw.predict(X_val_raw)\n",
    "mse_raw = mean_squared_error(y_val_raw, y_pred_raw)\n",
    "mae_raw = mean_absolute_error(y_val_raw, y_pred_raw)\n",
    "score_raw = rf_raw.score(X_val_raw, y_val_raw)\n",
    "print(f'Mean Squared Error for raw data: {mse_raw}')\n",
    "print(f'Mean Absolute Error for raw data: {mae_raw}')\n",
    "print(f'R^2 score for raw data: {score_raw}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T06:58:45.103420900Z",
     "start_time": "2024-02-18T06:58:44.499304100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# tune hyperparameters\n",
    "def grid_search_tunning(n_estimators, criterion, max_depth, min_samples_split, X_train, y_train, X_val, y_val):\n",
    "    \"\"\"\n",
    "    This function performs a grid search to find the best hyperparameters for a random forest regressor model.\n",
    "    params:\n",
    "    n_estimators: list of integers, the number of trees in the forest\n",
    "    criterion: list of strings, the function to measure the quality of a split\n",
    "    max_depth: list of integers, the maximum depth of the tree\n",
    "    min_samples_split: list of integers, the minimum number of samples required to split an internal node\n",
    "    X_train: pandas dataframe, the training data\n",
    "    y_train: pandas series, the training labels\n",
    "    X_val: pandas dataframe, the validation data\n",
    "    y_val: pandas series, the validation labels\n",
    "    \"\"\"\n",
    "    scores = []\n",
    "    scores_train = []\n",
    "    mse_list = []\n",
    "    mse_train_list = []\n",
    "    max_depths = []\n",
    "    min_samples_splits = []\n",
    "    min_mse = np.inf\n",
    "    min_mse_train = np.inf\n",
    "    max_score = -1\n",
    "    max_score_train = 0\n",
    "    for c in criterion:\n",
    "            for d in max_depth:\n",
    "                for s in min_samples_split:\n",
    "                    rf = RandomForestRegressor(n_estimators=n_estimators, criterion=c, max_depth=d, min_samples_split=s, random_state=42)\n",
    "                    rf.fit(X_train, y_train)\n",
    "                    # calculate performance using the validation set\n",
    "                    # let's start with testing data\n",
    "                    train_score = rf.score(X_train, y_train)\n",
    "                    mse_train = mean_squared_error(y_train, rf.predict(X_train))\n",
    "\n",
    "                    # calculate performance using the validation set\n",
    "                    score = rf.score(X_val, y_val)\n",
    "                    mse = mean_squared_error(y_val, rf.predict(X_val))\n",
    "                    min_samples_splits.append(s)\n",
    "                    max_depths.append(d)\n",
    "\n",
    "                    scores.append(score)\n",
    "                    scores_train.append(train_score)\n",
    "\n",
    "                    mse_list.append(mse)\n",
    "                    mse_train_list.append(mse_train)\n",
    "\n",
    "\n",
    "                    if mse < min_mse:\n",
    "                        best_mse = mse\n",
    "                        best_params_mse = {'n_estimators': n_estimators, 'criterion': c, 'max_depth': d, 'min_samples_split': s}\n",
    "                    if mse_train < min_mse_train:\n",
    "                        min_mse_train = mse_train\n",
    "                        best_params_mse_train = {'n_estimators': n_estimators, 'criterion': c, 'max_depth': d, 'min_samples_split': s}\n",
    "                    if score > max_score:\n",
    "                        max_score = score\n",
    "                        best_params_score = {'n_estimators': n_estimators, 'criterion': c, 'max_depth': d, 'min_samples_split': s}\n",
    "                    if train_score > max_score_train:\n",
    "                        max_score_train = train_score\n",
    "                        best_params_score_train = {'n_estimators': n_estimators, 'criterion': c, 'max_depth': d, 'min_samples_split': s}\n",
    "\n",
    "\n",
    "    print(f\"Best Parameters for Min MSE: {best_params_mse}\")\n",
    "    print(f\"Best Parameters for Max Score: {best_params_score}\")\n",
    "    print(f\"Best Parameters for Min MSE Train: {best_params_mse_train}\")\n",
    "    print(f\"Best parameters for max score train: {best_params_score_train}\")\n",
    "    return scores, scores_train, mse_list, mse_train_list, max_depths, min_samples_splits, best_params_mse, best_params_score, best_params_mse_train, best_params_score_train\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-18T08:07:02.542069100Z",
     "start_time": "2024-02-18T08:07:02.523316Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scores, scores_train, mse_list, mse_train_list, max_depths, min_samples_splits, best_params_mse, best_params_score, best_params_mse_train, best_params_score_train = grid_search_tunning(n_estimators=100,\n",
    "                    criterion=[\"squared_error\", \"friedman_mse\"],\n",
    "                    max_depth=[None, 2, 5, 10, 20, 30, 40],\n",
    "                    min_samples_split=[2, 5, 10, 20, 40],\n",
    "                    X_train=X_train, y_train=y_train,\n",
    "                    X_val=X_val,\n",
    "                    y_val=y_val)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-18T08:07:06.024508Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
